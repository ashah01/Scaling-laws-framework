2023/07/03

Changing the seed from 1 to 0 resulted in the dramatic improvement of trial 
(depth 9, wd 5e-4) - so much so that it went from breaking the functional form
to making it steeper! Now, the actual *research* part consists of 
understanding why it's [not] working, and fixing it when it's broken. Pulling
these threads is what allows a highly productive researcher at OpenAI to
identify and eliminate any irregularities, ensuring stability in results,
thereby producing a highly predictive functional form.

The answer lies in the learning curves of the variant trials. We must plot
the experiment in wandb (possibly including telemetry i.e. gradient norms), 
and analyze the results to understand the cause of variation. In doing so, we
can systematically construct a model with good scaling laws in addition to
simply executing the research well.

Observations
----

During the first couple of epochs, the most performant models in ascending
order are [7, 9, 5, 3, 2] - indicating that for deeper models, gains in 
performance take longer to be realized.

By step 17895, the scaling strategy starts to assume the form we'd expect -
as depth is scaled, performance follows. Suddenly, by step 19307, the form
degenerates into the ascending order of performance [2, 3, 7, 9, 5]. This is
kinda fixd by step 20722 when the order is [2, 3, 5, 9, 7]. At step 22134,
the ascending order of performance is [2, 3, 7, 9, 5]. Depth 9 assumes the
final best loss score, but the broader ascending order appears random at 
best.

Analysis
----


