At width 64, the model performs better with lower learning rates (3e-4, 1e-4).
Dropout is best left at 0
Depth doesn't seem to improve loss at width 64