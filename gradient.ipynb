{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pickle\n",
    "from argparse import Namespace\n",
    "import os\n",
    "import time\n",
    "import model\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # Random crop of size 32x32 with padding of 4 pixels\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.ToTensor()  # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data/CIFAR10\", train=True, download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data/CIFAR10\", train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    subdir = os.path.join(f\"./observations/{args.name}\", args.folder)\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=args.batch_size, shuffle=True\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=args.batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    net = getattr(model, args.name)(args.hidden_dim, args.depth)\n",
    "    net = net.to(device)\n",
    "\n",
    "    print(\"Number of parameters:\", sum([p.numel() for p in net.parameters()]))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = getattr(optim, args.optimizer)(net.parameters(), lr=args.lr, weight_decay=0.0001)\n",
    "    scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0, total_iters=len(trainloader)*args.epochs) # total_updates = (trainset / batch_size) * num_epochs\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    avg_test_losses = []\n",
    "    average_gradients = []\n",
    "    average_parameters = []\n",
    "    num_test_batches = math.ceil(10000 / args.batch_size)\n",
    "    time_start = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        for data in tqdm(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_scores.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            avg_grad_magnitude = 0.0\n",
    "            avg_param_magnitude = 0.0\n",
    "            total_parameters = 0\n",
    "            for name, param in net.named_parameters():\n",
    "                avg_param_magnitude += param.data.abs().sum().item()\n",
    "                avg_grad_magnitude += param.grad.abs().sum().item()\n",
    "                total_parameters += param.numel()\n",
    "            avg_grad_magnitude /= total_parameters\n",
    "            avg_param_magnitude /= total_parameters\n",
    "            average_gradients.append(avg_grad_magnitude)\n",
    "            average_parameters.append(avg_param_magnitude)\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = net(images)\n",
    "                l_test = criterion(outputs, labels)\n",
    "                test_scores.append(l_test.item())\n",
    "\n",
    "        avg_test_loss = sum(test_scores[-num_test_batches:]) / len(\n",
    "            test_scores[-num_test_batches:]\n",
    "        )\n",
    "\n",
    "        avg_test_losses.append(avg_test_loss)\n",
    "        print(avg_test_loss)\n",
    "\n",
    "    time_end = time.time()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(average_gradients)\n",
    "    ax2.plot(average_parameters)\n",
    "    ax1.set_title(\"Average Gradient Magnitude\")\n",
    "    ax2.set_title(\"Average Parameter Magnitude\")\n",
    "    fig.savefig(f\"observations/{args.name}/{args.folder}/{args.optimizer}{args.depth}.png\")   # save the figure to file\n",
    "    plt.close(fig)    # close the figure window\n",
    "\n",
    "    running_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = torch.nn.functional.log_softmax(net(images), dim=1)\n",
    "            running_sum += (outputs.argmax(dim=1) != labels).sum().item()\n",
    "\n",
    "    print(\"Error %: \", (running_sum / len(testset)))\n",
    "\n",
    "    if args.save:\n",
    "        with open(\n",
    "            f\"observations/{args.name}/{args.folder}/train_scores_b{args.batch_size}dr{args.dropout}lr{args.lr}d{args.depth}w{args.hidden_dim}\",\n",
    "            \"wb\",\n",
    "        ) as f:\n",
    "            pickle.dump(train_scores, f)\n",
    "            f.close()\n",
    "        with open(\n",
    "            f\"observations/{args.name}/{args.folder}/test_scores_b{args.batch_size}dr{args.dropout}lr{args.lr}d{args.depth}w{args.hidden_dim}\",\n",
    "            \"wb\",\n",
    "        ) as f:\n",
    "            pickle.dump(test_scores, f)\n",
    "            f.close()\n",
    "    if args.log:\n",
    "        with open(f\"observations/{args.name}/{args.folder}/analytics.txt\", \"a\") as f:\n",
    "            f.write(\n",
    "                f\"batch size: {args.batch_size}, lr: {args.lr}, hidden dim: {args.hidden_dim}, depth: {args.depth}, params: {sum([p.numel() for p in net.parameters()])}, dropout: {args.dropout}, loss: {min(avg_test_losses)}, error %: {running_sum / len(testset)}, time: {time_end - time_start}, epochs: {args.epochs}\\n\"\n",
    "            )\n",
    "            f.close()\n",
    "\n",
    "\n",
    "for optimizer in [\"Adam\", \"AdamW\"]:\n",
    "    for depths in [5, 9]:\n",
    "        train(\n",
    "            Namespace(\n",
    "                name=\"ResNet\",\n",
    "                epochs=15,\n",
    "                batch_size=128,\n",
    "                lr=0.01,\n",
    "                optimizer=optimizer,\n",
    "                hidden_dim=16,\n",
    "                depth=depths,\n",
    "                dropout=0,\n",
    "                save=True,\n",
    "                log=True,\n",
    "                folder=\"joshgradient\",\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loss.backward, the gradients are stored in the .grad attribute of each layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist-scaling-laws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
